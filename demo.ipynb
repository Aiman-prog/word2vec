{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00",
   "metadata": {},
   "source": [
    "# Word2Vec from Scratch (Skip-Gram + Negative Sampling)\n",
    "Pure NumPy implementation. Skip-gram with negative sampling (SGNS) trained on text8."
   ]
  },
  {
   "cell_type": "code",
   "id": "c01",
   "metadata": {},
   "source": [
    "%pip install numpy matplotlib"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c02",
   "metadata": {},
   "source": [
    "## Smoke Test — Tiny Corpus\n",
    "Trains on a 6-word sentence to verify the full pipeline works end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "id": "c03",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from main import train\n",
    "from evaluate import cosine_similarity\n",
    "\n",
    "W, w2i, i2w, losses = train(\"the cat sat on the mat\",\n",
    "                             embedding_dim=5, window_size=1,\n",
    "                             num_negatives=2, learning_rate=0.1,\n",
    "                             num_epochs=100, min_count=1, batch_size=32)\n",
    "\n",
    "print(f\"cat ↔ on  : {cosine_similarity(W[w2i['cat']], W[w2i['on']]):.4f}  (share 2 contexts)\")\n",
    "print(f\"cat ↔ mat : {cosine_similarity(W[w2i['cat']], W[w2i['mat']]):.4f}  (share 1 context)\")\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(range(1, len(losses) + 1), losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg loss\")\n",
    "plt.title(\"Training loss — tiny corpus\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c04",
   "metadata": {},
   "source": [
    "## Full Training on text8\n",
    "~17M tokens, vocab ~71k words. Trains in ~15–20 min on CPU.  \n",
    "Saved model is loaded automatically if already trained."
   ]
  },
  {
   "cell_type": "code",
   "id": "c05",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from main import train, save_model, load_model\n",
    "\n",
    "MODEL_PATH  = \"model/model\"      # model/model.npy + model/model.json\n",
    "LOSSES_PATH = \"model/losses.json\"\n",
    "CORPUS_FILE = \"text8\"\n",
    "\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "if os.path.exists(f\"{MODEL_PATH}.npy\"):\n",
    "    W, w2i, i2w = load_model(MODEL_PATH)\n",
    "    with open(LOSSES_PATH) as f:\n",
    "        losses = json.load(f)\n",
    "else:\n",
    "    with open(CORPUS_FILE) as f:\n",
    "        corpus = f.read()\n",
    "    W, w2i, i2w, losses = train(corpus, embedding_dim=100, window_size=5,\n",
    "                                 num_negatives=5, learning_rate=0.025, num_epochs=5,\n",
    "                                 batch_size=512)\n",
    "    save_model(W, w2i, MODEL_PATH)\n",
    "    with open(LOSSES_PATH, \"w\") as f:\n",
    "        json.dump(losses, f)\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(range(1, len(losses) + 1), losses, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Avg loss\")\n",
    "plt.title(\"Training loss — text8\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c06",
   "metadata": {},
   "source": [
    "## Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "id": "c07",
   "metadata": {},
   "source": [
    "from evaluate import find_nearest, normalize_embeddings\n",
    "\n",
    "W_normed = normalize_embeddings(W)\n",
    "for word in [\"king\", \"woman\", \"computer\"]:\n",
    "    if word in w2i:\n",
    "        neighbours = find_nearest(word, w2i, i2w, W_normed)\n",
    "        print(f\"{word}: {[w for w, _ in neighbours]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c08",
   "metadata": {},
   "source": [
    "## Analogies  (a : b :: c : ?)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c09",
   "metadata": {},
   "source": [
    "from evaluate import analogy\n",
    "\n",
    "tests = [(\"man\", \"king\", \"woman\"), (\"france\", \"paris\", \"england\"), (\"good\", \"better\", \"bad\")]\n",
    "for a, b, c in tests:\n",
    "    if all(w in w2i for w in (a, b, c)):\n",
    "        print(f\"{a} : {b}  ::  {c} : {analogy(a, b, c, w2i, i2w, W_normed)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c10",
   "metadata": {},
   "source": [
    "## Analogy Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "c11",
   "metadata": {},
   "source": [
    "from evaluate import eval_analogies\n",
    "\n",
    "ANALOGY_TESTS = [\n",
    "    # semantic — capitals\n",
    "    (\"france\",   \"paris\",    \"england\",  \"london\"),\n",
    "    (\"germany\",  \"berlin\",   \"france\",   \"paris\"),\n",
    "    (\"italy\",    \"rome\",     \"france\",   \"paris\"),\n",
    "    # semantic — gender\n",
    "    (\"man\",      \"king\",     \"woman\",    \"queen\"),\n",
    "    (\"man\",      \"actor\",    \"woman\",    \"actress\"),\n",
    "    (\"man\",      \"father\",   \"woman\",    \"mother\"),\n",
    "    (\"man\",      \"brother\",  \"woman\",    \"sister\"),\n",
    "    # semantic — comparative\n",
    "    (\"good\",     \"better\",   \"bad\",      \"worse\"),\n",
    "    (\"great\",    \"greater\",  \"small\",    \"smaller\"),\n",
    "    (\"big\",      \"bigger\",   \"small\",    \"smaller\"),\n",
    "    # semantic — verb tense\n",
    "    (\"walk\",     \"walked\",   \"run\",      \"ran\"),\n",
    "    (\"go\",       \"went\",     \"buy\",      \"bought\"),\n",
    "    # semantic — plurals\n",
    "    (\"man\",      \"men\",      \"woman\",    \"women\"),\n",
    "    (\"child\",    \"children\", \"dog\",      \"dogs\"),\n",
    "]\n",
    "\n",
    "acc, correct, total = eval_analogies(ANALOGY_TESTS, w2i, i2w, W_normed)\n",
    "print(f\"Analogy accuracy: {correct}/{total}  ({acc:.1%})\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}